{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9523d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from SentimentAnalysis import *\n",
    "from TextAnalytics import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover,Tokenizer, RegexTokenizer, CountVectorizer, IDF\n",
    "from pyspark.sql.functions import rank,udf, col, size, explode, regexp_replace, trim, lower, lit\n",
    "from pyspark.sql.types import *\n",
    "# from pyspark.ml.clustering import LDA\n",
    "import pyLDAvis\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "# from pyspark.sql.functions import rank, col\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "import plotly\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from shapely import geometry\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import folium\n",
    "from geopandas import sjoin\n",
    "import json\n",
    "import geojson\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60983ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    data ={\"tweet_id\":[], \"time\":[],\"text\":[],\"city\":[],\"coordinates\":[],\"react_count\":[],\"quote_count\":[],\"reply_count\":[],\n",
    "           \"retweet_count\":[],\"favorite_count\":[]}\n",
    "\n",
    "    read_files = glob.glob(path)\n",
    "    for file in read_files:\n",
    "        with open(str(file), 'r') as f:\n",
    "            objects = ijson.items(f, 'results.item')\n",
    "            for obj in objects:\n",
    "                for row in obj:\n",
    "                    if row['text'].startswith(\"RT\") == False:\n",
    "                        data['tweet_id'].append(row['id'])\n",
    "                        data['time'].append(row['created_at'])\n",
    "                        if \"extended_tweet\" in row:\n",
    "                            data['text'].append(row[\"extended_tweet\"]['full_text'])\n",
    "                        else:\n",
    "                            data['text'].append(row[\"text\"])\n",
    "                        data['city'].append(row['place']['name'])\n",
    "                        data['coordinates'].append(row['coordinates'])\n",
    "                        data['quote_count'].append(row['quote_count'])\n",
    "                        data['reply_count'].append(row['reply_count'])\n",
    "                        data['retweet_count'].append(row['retweet_count'])\n",
    "                        data['favorite_count'].append(row['favorite_count'])\n",
    "                        data['react_count'].append(row['quote_count']+row['reply_count']+row['retweet_count']+row['favorite_count'])\n",
    "    data = pd.DataFrame(data)\n",
    "    data['time'] = pd.to_datetime(data['time']).dt.date\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bac8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_df(data):\n",
    "    spark = SparkSession.builder.appName('NLP').getOrCreate()\n",
    "    df = spark.createDataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc610646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_word(text):\n",
    "    return text.split(\" \")\n",
    "\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
    "fix_abbreviation_udf = udf(fix_abbreviation, StringType())\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())\n",
    "sentiment_udf = udf(lambda x: condition(x), StringType())\n",
    "\n",
    "check_blanks_udf = udf(check_blanks, StringType())\n",
    "check_lang_udf = udf(check_lang, StringType())\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(lemmatize, StringType())\n",
    "to_word_udf = udf(to_word,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic_analysis():\n",
    "    def __init__(self,rawdata):\n",
    "        raw_cols =  rawdata.columns\n",
    "        data = rawdata.withColumn('non_asci', strip_non_ascii_udf(rawdata['text']))\n",
    "        data = data.select(raw_cols+['non_asci'])\\\n",
    "                 .withColumn('fixed_abbrev',fix_abbreviation_udf(data['non_asci']))\n",
    "        data = data.select(raw_cols+['fixed_abbrev'])\\\n",
    "                .withColumn('stop_texts',remove_stops_udf(data['fixed_abbrev']))\n",
    "        data= data.select(raw_cols+['stop_texts'])\\\n",
    "                .withColumn('removed',remove_features_udf(data['stop_texts']))\n",
    "        data = data.select(raw_cols+['removed'])\\\n",
    "                  .withColumn('tagged_text',tag_and_remove_udf(data['removed']))\n",
    "        data = data.select(raw_cols+['tagged_text']) \\\n",
    "                  .withColumn('lemm_text',lemmatize_udf(data['tagged_text']))\n",
    "        \n",
    "        data = data.select(raw_cols+['lemm_text']) \\\n",
    "                  .withColumn(\"is_blank\", check_blanks_udf(data[\"lemm_text\"]))\n",
    "        data = data.select(raw_cols+['lemm_text','is_blank']) \\\n",
    "                  .withColumn(\"word\", to_word_udf(data[\"lemm_text\"]))\n",
    "        \n",
    "        data = data.withColumn(\"uid\", monotonically_increasing_id())\n",
    "        data = data.filter(data[\"is_blank\"] == \"False\")\n",
    "        \n",
    "        self.clean_data = data\n",
    "        \n",
    "        self.data = rawdata\n",
    "        \n",
    "        self.stopwords = ['love','week','way','watch','fuck','use','be','want','good','great','thanks','amp','see','go','think','people','today','say','get','time','day','look','make','know','need','thank','come','do','new','take','thing','take','make','know','need','new','year','many']\n",
    "    def lda(self):\n",
    "        def indices_to_terms_udf(vocabulary):\n",
    "            def indices_to_terms(xs):\n",
    "                return [vocabulary[int(x)] for x in xs]\n",
    "            return udf(indices_to_terms, ArrayType(StringType()))\n",
    "    \n",
    "        def format_data_to_pyldavis(df_filtered, count_vectorizer, transformed, lda_model):\n",
    "            xxx = df_filtered.select((explode(df_filtered.word)).alias(\"words\")).groupby(\"words\").count()\n",
    "            word_counts = {r['words']:r['count'] for r in xxx.collect()}\n",
    "            word_counts = [word_counts[w] for w in count_vectorizer.vocabulary]\n",
    "\n",
    "            data = {'topic_term_dists': np.array(lda_model.topicsMatrix().toArray()).T, \n",
    "                    'doc_topic_dists': np.array([x.toArray() for x in transformed.select([\"topicDistribution\"]).toPandas()['topicDistribution']]),\n",
    "                    'doc_lengths': [r[0] for r in df_filtered.select(size(df_filtered.word)).collect()],\n",
    "                    'vocab': count_vectorizer.vocabulary,\n",
    "                    'term_frequency': word_counts}\n",
    "\n",
    "            return data\n",
    "\n",
    "        def filter_bad_docs(data):\n",
    "            bad = 0\n",
    "            doc_topic_dists_filtrado = []\n",
    "            doc_lengths_filtrado = []\n",
    "\n",
    "            for x,y in zip(data['doc_topic_dists'], data['doc_lengths']):\n",
    "                if np.sum(x)==0:\n",
    "                    bad+=1\n",
    "                elif np.sum(x) != 1:\n",
    "                    bad+=1\n",
    "                elif np.isnan(x).any():\n",
    "                    bad+=1\n",
    "                else:\n",
    "                    doc_topic_dists_filtrado.append(x)\n",
    "                    doc_lengths_filtrado.append(y)\n",
    "\n",
    "            data['doc_topic_dists'] = doc_topic_dists_filtrado\n",
    "            data['doc_lengths'] = doc_lengths_filtrado\n",
    "            return data\n",
    "        \n",
    "        # Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "        tokenizer = Tokenizer(inputCol=\"lemm_text\", outputCol=\"words\")\n",
    "        #data = tokenizer.transform(data)\n",
    "        vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
    "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "        #idfModel = idf.fit(data)\n",
    "\n",
    "        lda = LDA(k=20, seed=1, optimizer=\"em\")\n",
    "\n",
    "        cv = CountVectorizer(inputCol=\"word\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "        cvmodel = cv.fit(self.clean_data)\n",
    "        featurizedData = cvmodel.transform(self.clean_data)\n",
    "        \n",
    "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "        idfModel = idf.fit(featurizedData)\n",
    "        rescaledData = idfModel.transform(featurizedData) # TFIDF\n",
    "        \n",
    "        lda = LDA(k=6, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "        ldamodel = lda.fit(rescaledData)\n",
    "        ldatopics = ldamodel.describeTopics()\n",
    "        \n",
    "        ldatopics = ldatopics.withColumn(\n",
    "            \"topics_words\", indices_to_terms_udf(cvmodel.vocabulary)(\"termIndices\"))\n",
    "        \n",
    "        transformed = ldamodel.transform(rescaledData)\n",
    "        formatted = format_data_to_pyldavis(self.clean_data, cvmodel, transformed, ldamodel)\n",
    "        final = filter_bad_docs(formatted)\n",
    "        \n",
    "        return final\n",
    "\n",
    "    def top_react_tweet(self):\n",
    "        df = self.clean_data.select('*',month('time').alias('month'))\n",
    "        df = df.orderBy(df.month,df.react_count.desc())\n",
    "        window = Window.partitionBy(df['month']).orderBy(df['react_count'].desc())\n",
    "        df_pd = df.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 10).orderBy('month')\n",
    "        df_pd = df_pd.select('text','word','react_count','quote_count','reply_count','retweet_count','favorite_count','month','rank').toPandas()\n",
    "        return df_pd\n",
    "    \n",
    "    def word_cloud_fall(self,input_month):\n",
    "        months = {\"March\":3,\"April\":4,\"May\":5,\"June\":6,\"July\":7,\"August\":8,\"September\":9,\"October\":10,\"November\":11,\"December\":12}\n",
    "        word = self.clean_data.select(month('time').alias('month'),'lemm_text')\n",
    "        words_byMonth = word.groupby('month').agg(collect_list('lemm_text').alias(\"words\")).orderBy('month')\n",
    "\n",
    "        def tweets_string(text):\n",
    "            return ' '.join(text)\n",
    "\n",
    "        tweets_string_udf = udf(tweets_string, StringType())\n",
    "        words_byMonth = words_byMonth.select(\"*\")\\\n",
    "                          .withColumn('all_words',tweets_string_udf(words_byMonth['words']))\n",
    "\n",
    "        colosseum_mask = np.array(Image.open('VIC_shape.jpeg'))\n",
    "        colors = ImageColorGenerator(colosseum_mask)\n",
    "\n",
    "        monthly_word = words_byMonth.filter(words_byMonth.month==months[input_month]).select('all_words').collect()[0].all_words\n",
    "        return monthly_word\n",
    "#         cloud = WordCloud(mask=colosseum_mask,\n",
    "#                           stopwords=self.stopwords,\n",
    "#                           background_color='white',\n",
    "#                           colormap='tab20b',\n",
    "#                             ).generate_from_text(monthly_word)\n",
    "#         file_name = 'Chart_HTML/wordCloud_{}.png'.format(input_month)\n",
    "#         plt.figure(figsize=(18,12))\n",
    "#         plt.imshow(cloud)\n",
    "#         plt.axis('off')\n",
    "#         plt.title(input_month,fontsize=20)\n",
    "#         plt.savefig(file_name)\n",
    "        \n",
    "    def word_cloud_spring(self,input_month):\n",
    "        months = {\"March\":3,\"April\":4,\"May\":5,\"June\":6,\"July\":7,\"August\":8,\"September\":9,\"October\":10,\"November\":11,\"December\":12}\n",
    "        word = self.clean_data.select(month('time').alias('month'),'lemm_text')\n",
    "        words_byMonth = word.groupby('month').agg(collect_list('lemm_text').alias(\"words\")).orderBy('month')\n",
    "\n",
    "        def tweets_string(text):\n",
    "            return ' '.join(text)\n",
    "\n",
    "        tweets_string_udf = udf(tweets_string, StringType())\n",
    "        words_byMonth = words_byMonth.select(\"*\")\\\n",
    "                          .withColumn('all_words',tweets_string_udf(words_byMonth['words']))\n",
    "\n",
    "        colosseum_mask = np.array(Image.open('VIC_shape.jpeg'))\n",
    "        colors = ImageColorGenerator(colosseum_mask)\n",
    "\n",
    "        monthly_word = words_byMonth.filter(words_byMonth.month==months[input_month]).select('all_words').collect()[0].all_words\n",
    "        cloud = WordCloud(mask=colosseum_mask,\n",
    "                          stopwords=self.stopwords,\n",
    "                          background_color='white',\n",
    "                          colormap='tab20',\n",
    "                            ).generate_from_text(monthly_word)\n",
    "        file_name = 'Chart_HTML/wordCloud_{}.png'.format(input_month)\n",
    "        plt.figure(figsize=(18,12))\n",
    "        plt.imshow(cloud)\n",
    "        plt.axis('off')\n",
    "        plt.title(input_month,fontsize=20)\n",
    "        plt.savefig(file_name)\n",
    "    def word_cloud_winter(self,input_month):\n",
    "        months = {\"March\":3,\"April\":4,\"May\":5,\"June\":6,\"July\":7,\"August\":8,\"September\":9,\"October\":10,\"November\":11,\"December\":12}\n",
    "        word = self.clean_data.select(month('time').alias('month'),'lemm_text')\n",
    "        words_byMonth = word.groupby('month').agg(collect_list('lemm_text').alias(\"words\")).orderBy('month')\n",
    "\n",
    "        def tweets_string(text):\n",
    "            return ' '.join(text)\n",
    "\n",
    "        tweets_string_udf = udf(tweets_string, StringType())\n",
    "        words_byMonth = words_byMonth.select(\"*\")\\\n",
    "                          .withColumn('all_words',tweets_string_udf(words_byMonth['words']))\n",
    "\n",
    "        colosseum_mask = np.array(Image.open('VIC_shape.jpeg'))\n",
    "        colors = ImageColorGenerator(colosseum_mask)\n",
    "\n",
    "        monthly_word = words_byMonth.filter(words_byMonth.month==months[input_month]).select('all_words').collect()[0].all_words\n",
    "        cloud = WordCloud(mask=colosseum_mask,\n",
    "                          stopwords=self.stopwords,\n",
    "                          background_color='white',\n",
    "                          colormap='ocean',\n",
    "                            ).generate_from_text(monthly_word)\n",
    "        file_name = 'Chart_HTML/wordCloud_{}.png'.format(input_month)\n",
    "        plt.figure(figsize=(18,12))\n",
    "        plt.imshow(cloud)\n",
    "        plt.axis('off')\n",
    "        plt.title(input_month,fontsize=20)\n",
    "        plt.savefig(file_name)\n",
    "    def word_cloud_summer(self,input_month):\n",
    "        months = {\"March\":3,\"April\":4,\"May\":5,\"June\":6,\"July\":7,\"August\":8,\"September\":9,\"October\":10,\"November\":11,\"December\":12}\n",
    "        word = self.clean_data.select(month('time').alias('month'),'lemm_text')\n",
    "        words_byMonth = word.groupby('month').agg(collect_list('lemm_text').alias(\"words\")).orderBy('month')\n",
    "\n",
    "        def tweets_string(text):\n",
    "            return ' '.join(text)\n",
    "\n",
    "        tweets_string_udf = udf(tweets_string, StringType())\n",
    "        words_byMonth = words_byMonth.select(\"*\")\\\n",
    "                          .withColumn('all_words',tweets_string_udf(words_byMonth['words']))\n",
    "\n",
    "        colosseum_mask = np.array(Image.open('VIC_shape.jpeg'))\n",
    "        colors = ImageColorGenerator(colosseum_mask)\n",
    "        \n",
    "        monthly_word = words_byMonth.filter(words_byMonth.month==months[input_month]).select('all_words').collect()[0].all_words\n",
    "        cloud = WordCloud(mask=colosseum_mask,\n",
    "                          stopwords=self.stopwords,\n",
    "                          background_color='white',\n",
    "                          colormap='Set1',\n",
    "                            ).generate_from_text(monthly_word)\n",
    "        file_name = 'Chart_HTML/WordCloud/wordCloud_{}.png'.format(input_month)\n",
    "        plt.figure(figsize=(18,12))\n",
    "        plt.imshow(cloud)\n",
    "        plt.axis('off')\n",
    "        plt.title(input_month,fontsize=20)\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e52c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    def __init__(self,df):\n",
    "        df = df.withColumn('text_non_asci',strip_non_ascii_udf(df['text']))\n",
    "        df = df.withColumn('fixed_abbrev',fix_abbreviation_udf(df['text_non_asci']))\n",
    "        df = df.withColumn('removed',remove_features_udf(df['fixed_abbrev']))\n",
    "        df = df.withColumn(\"sentiment_score\", sentiment_analysis_udf( df['removed'] ))\n",
    "        df = df.withColumn(\"sentiment\", sentiment_udf( df['sentiment_score'] ))\n",
    "        self.SA_results = df\n",
    "\n",
    "        covid_relate = df.withColumn('covid_content',when((col(\"removed\").like('%covid%') | col(\"removed\").like('%coronavirus%')),\"Covid-related\").otherwise(\"Not covid-related\"))\n",
    "        self.covid_relate_date = covid_relate.select('text','time',month('time').alias('month'),'sentiment','covid_content')\n",
    "        self.covid_relate  = self.covid_relate_date.groupBy('month','sentiment','covid_content').count().orderBy('month','covid_content')\n",
    "\n",
    "        summary = self.covid_relate.orderBy('month','covid_content').toPandas()\n",
    "        freq = self.covid_relate_date.groupBy('month','covid_content').count().orderBy('month','covid_content').toPandas()\n",
    "        summary['total']=list(itertools.chain.from_iterable(itertools.repeat(x, 3) for x in freq['count']))\n",
    "        summary['percentage'] = summary['count']/summary['total']*100\n",
    "        summary['percentage'] = summary['percentage'].round(decimals=1)\n",
    "        summary['month'] = summary['month'].replace({3:\"March\",4:\"April\",5:\"May\",6:\"June\",7:\"July\",8:\"August\",9:\"September\",10:\"October\",11:\"November\",12:\"December\"})\n",
    "        self.summary = summary\n",
    "\n",
    "        month_count_covid = self.covid_relate.filter(covid_relate.covid_content==\"Covid-related\").groupBy('month').sum('count').orderBy('month')\n",
    "        month_covid = month_count_covid.toPandas().rename(columns={'month':'Month','sum(count)':'Number of tweets'})\n",
    "        month_covid['Month'] = month_covid['Month'].replace({3:\"March\",4:\"April\",5:\"May\",6:\"June\",7:\"July\",8:\"August\",9:\"September\",10:\"October\",11:\"November\",12:\"December\"})\n",
    "        self.month_covid = month_covid\n",
    "    \n",
    "    def sentiment_results(self):\n",
    "        return self.SA_results\n",
    "    def print_summary(self):\n",
    "        return self.summary\n",
    "    def by_city(self):\n",
    "        city_score = self.SA_results.select('city','time',month('time').alias('month'),'sentiment_score').groupBy('city','month').agg(avg(col(\"sentiment_score\"))).orderBy('city','month')\n",
    "\n",
    "        return city_score\n",
    "    \n",
    "    def covid_overall_chart(self):\n",
    "#         fig = px.bar(self.summary, x=\"sentiment\", y=\"percentage\",color=\"sentiment\", facet_row=\"covid_content\", facet_col=\"month\",\n",
    "#                           category_orders = {\"sentiment\":[\"positive\",\"neutral\",\"negative\"]},\n",
    "#                     labels={\n",
    "#                          \"percentage %\": \"Percentage %\",\n",
    "#                          \"sentiment\": \"Sentiment\",\n",
    "#                          \"month\": \"Month\"})\n",
    "        fig = px.bar(self.summary, x=\"covid_content\", y=\"percentage\",color=\"sentiment\",barmode='group',\n",
    "                  animation_frame=\"month\", animation_group=\"covid_content\",color_discrete_sequence=['rgb(255,217,47)', '#3283FE','rgb(179,179,179)'],\n",
    "                 category_orders = {\"sentiment\":[\"positive\",\"neutral\",\"negative\"]},\n",
    "            labels={\n",
    "                 \"percentage %\": \"Percentage %\",\n",
    "                 \"sentiment\": \"Sentiment\",\n",
    "                 \"month\": \"Month\"})\n",
    "        fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "        for axis in fig.layout:\n",
    "            if type(fig.layout[axis]) == go.layout.YAxis:\n",
    "                fig.layout[axis].title.text = ''\n",
    "            if type(fig.layout[axis]) == go.layout.XAxis:\n",
    "                fig.layout[axis].title.text = ''\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                    'text':\"Sentiment of tweets in 2020\",\n",
    "                    'y':0.98,\n",
    "                    'x':0.5,\n",
    "                    'xanchor': 'center',\n",
    "                    'yanchor': 'top'},\n",
    "             autosize=False,\n",
    "                width=650,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            ),\n",
    "            template=\"plotly_white\",\n",
    "            annotations = list(fig.layout.annotations) + \n",
    "                [go.layout.Annotation(\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        font=dict(\n",
    "                            size=14\n",
    "                        ),\n",
    "                        showarrow=False,\n",
    "                        text=\"Percentage\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        fig.update_xaxes(matches='x')\n",
    "        fig.update_yaxes(matches=None,ticksuffix=\"%\")\n",
    "        fig.update_traces(texttemplate='%{y}', textposition='inside')\n",
    "        \n",
    "        fig.write_json(\"graph_json/monthly_covidTweets_Bar.json\")\n",
    "#         fig.write_html(\"Chart_HTML/Covid-related_glance/monthly_covidTweets_Bar.html\")\n",
    "        \n",
    "        annotations=[]\n",
    "        annotations.append(dict(xref='paper', yref='paper',\n",
    "          x=0.5, y=0.5,\n",
    "          xanchor= 'center',\n",
    "          yanchor='middle',\n",
    "          text= '<b>COVID-19<b>',\n",
    "          font=dict(family=\"Arial\", size=30),\n",
    "          showarrow=False,\n",
    "          ))\n",
    "\n",
    "        pie = go.Figure(data=[go.Pie(labels=self.month_covid['Month'], values=self.month_covid['Number of tweets'], hole=.4,\n",
    "                                     direction ='clockwise', sort=False,\n",
    "                                     marker=dict(colors=px.colors.sequential.haline,\n",
    "                                                line=dict(color='white',width=2)),\n",
    "                                    )])\n",
    "\n",
    "        pie.update_traces(\n",
    "          textposition='inside',\n",
    "          textinfo='value+percent',\n",
    "          showlegend=True,\n",
    "        )\n",
    "\n",
    "        pie.update_layout(\n",
    "        title={\n",
    "                'text':\"Distribution of Covid-related tweets over the pandemic period\",\n",
    "                'y':0.95,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'},\n",
    "         annotations=annotations,\n",
    "         autosize=False,\n",
    "            width=700,\n",
    "            height=700,\n",
    "        )\n",
    "        pie.write_json(\"graph_json/monthly_dist_pie.json\")\n",
    "#         pie.write_html(\"Chart_HTML/Covid-related_glance/monthly_dist_pie.html\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e29680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_impact:\n",
    "    def __init__(self,SA_results):\n",
    "        self.ASX = pd.read_json('ASX200.json')\n",
    "        self.ASX['Price'] = pd.to_numeric(self.ASX['Price'].str.replace(\",\", \"\"))\n",
    "        \n",
    "        self.SP500 = pd.read_json('SP500.json')\n",
    "        self.SP500['Price'] = pd.to_numeric(self.SP500['Price'].str.replace(\",\", \"\"))\n",
    "        \n",
    "        self.AUD_index = pd.read_json('AUD_index.json')\n",
    "        self.AUD_index['Price'] = pd.to_numeric(self.AUD_index['Price'])*100\n",
    "        \n",
    "        self.raw_SA = SA_results.select('time','sentiment_score','sentiment').toPandas()\n",
    "        mean_sentiment = SA_results.select('time',dayofmonth('time').alias('day'),month('time').alias('month'),'sentiment_score','sentiment').groupBy('time','month','day').agg(avg(col(\"sentiment_score\"))).orderBy('time')\n",
    "        mean_sentiment = mean_sentiment.toPandas()\n",
    "        mean_sentiment['month'] = mean_sentiment['month'].replace({3:\"March\",4:\"April\",5:\"May\",6:\"June\",7:\"July\",8:\"August\",9:\"September\",11:\"November\"})\n",
    "        mean_sentiment['scaled_mean'] = preprocessing.scale(mean_sentiment['avg(sentiment_score)'])\n",
    "        self.mean_sentiment = mean_sentiment\n",
    "\n",
    "        self.by_week =SA_results.withColumn('week_of_year', weekofyear('time')).withColumn(\"week_strt_day\",date_sub(next_day(col(\"time\"),\"monday\"),1)).toPandas()\n",
    "        self.by_week['scaled_score'] = preprocessing.scale(self.by_week['sentiment_score'])\n",
    "\n",
    "        confirmed_df = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n",
    "        confirmed_df = confirmed_df[confirmed_df['Country/Region']==\"Australia\"].drop(['Country/Region','Lat','Long'],1)\n",
    "        confirmed_df = confirmed_df.set_index(\"Province/State\").transpose().reset_index()\n",
    "        confirmed_df = confirmed_df.rename(columns = {'index':'Date','Victoria':'VIC_confirmed'})\n",
    "        confirmed_df['Date'] = pd.to_datetime(confirmed_df['Date']).dt.date\n",
    "        \n",
    "        death_df = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\")\n",
    "        death_df = death_df[death_df['Country/Region']==\"Australia\"].drop(['Country/Region','Lat','Long'],1)\n",
    "        death_df = death_df.set_index(\"Province/State\").transpose().reset_index()\n",
    "        death_df = death_df.rename(columns = {'index':'Date','Victoria':'VIC_death'})\n",
    "        \n",
    "        death_df['Date'] = pd.to_datetime(death_df['Date']).dt.date\n",
    "        recover_df = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\")\n",
    "        recover_df  = recover_df [recover_df['Country/Region']==\"Australia\"].drop(['Country/Region','Lat','Long'],1)\n",
    "        recover_df = recover_df.set_index(\"Province/State\").transpose().reset_index()\n",
    "        recover_df = recover_df.rename(columns = {'index':'Date','Victoria':'VIC_recover'})\n",
    "        recover_df['Date'] = pd.to_datetime(recover_df['Date']).dt.date\n",
    "        \n",
    "        summary = pd.merge(pd.merge(confirmed_df,recover_df,on=\"Date\"),death_df,on=\"Date\")\n",
    "        summary['VIC_active'] = summary['VIC_confirmed']-summary['VIC_death']-summary['VIC_recover']\n",
    "        self.summary = summary\n",
    "\n",
    "    def overall(self):\n",
    "\n",
    "        bar = make_subplots()\n",
    "#         bar.update_layout(\n",
    "#                 template=\"plotly_white\",\n",
    "#                 title={\n",
    "#                     'text':\"Trends of sentiment scores and Australian stock market\",\n",
    "#                     'y':0.98,\n",
    "#                     'x':0.5,\n",
    "#                     'xanchor': 'center',\n",
    "#                     'yanchor': 'top'},\n",
    "#                 legend=dict(\n",
    "#                 orientation=\"h\",yanchor=\"bottom\",y=1.1),\n",
    "#                          yaxis_range=[-9,11])\n",
    "      \n",
    "        bar.add_trace(go.Bar(x=self.summary['Date'],y=self.summary['VIC_active'],name='Active cases'))\n",
    "        bar.add_trace(go.Scatter(x=self.summary['Date'], y=self.summary['VIC_death'],\n",
    "                    mode='lines',\n",
    "                    name='Death cases',visible=\"legendonly\"))\n",
    "        bar.add_trace(go.Scatter(x=self.summary['Date'], y=self.summary['VIC_confirmed'],\n",
    "                    mode='lines',\n",
    "                    name='Confirmed cases',visible=\"legendonly\"))\n",
    "        bar.add_trace(go.Scatter(x=self.summary['Date'], y=self.summary['VIC_recover'],\n",
    "                    mode='lines',\n",
    "                    name='Recover cases',visible=\"legendonly\"))\n",
    "#         bar = go.Figure(\n",
    "#             data=[go.Bar(x=self.summary['Date'],y=self.summary['VIC_active'])]\n",
    "#         )\n",
    "        bar.update_layout(\n",
    "            template=\"plotly_white\",\n",
    "            title={\n",
    "                'text': \"Covid-19 Cases in VIC\",\n",
    "                'y':0.9,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'})\n",
    "        \n",
    "        bar.update_xaxes(\n",
    "            rangeslider_visible=True,\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1 month\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6 months\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"1 year\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        \n",
    "#         bar.write_html(\"Chart_HTML/Covid-related_glance/covidCases_bar.html\")\n",
    "        bar.write_json(\"graph_json/covidCases_bar.json\")\n",
    "        \n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        fig.update_layout(xaxis2= {'anchor': 'y', 'overlaying': 'x', 'side': 'top'},\n",
    "                template=\"plotly_white\",\n",
    "                title={\n",
    "                    'text':\"Trends of sentiment scores and Australian stock market\",\n",
    "                    'y':0.98,\n",
    "                    'x':0.5,\n",
    "                    'xanchor': 'center',\n",
    "                    'yanchor': 'top'},\n",
    "                legend=dict(\n",
    "                orientation=\"h\",yanchor=\"bottom\",y=1.1),\n",
    "                         yaxis_range=[-9,11])\n",
    "      \n",
    "        fig.add_trace(go.Box(x=self.by_week.week_of_year,\n",
    "            y=self.by_week.scaled_score,\n",
    "            boxpoints=False, # no data points\n",
    "            marker_color='rgb(9,56,125)',\n",
    "            line_color='rgb(9,56,125)',\n",
    "            name='Distribution of sentiment scores'),secondary_y=False)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=self.mean_sentiment.time, y=self.mean_sentiment['scaled_mean'],\n",
    "            mode='lines',\n",
    "            name='Average sentiment score'),secondary_y=False)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=self.ASX.Date, y=self.ASX['Price'],\n",
    "                    mode='lines',\n",
    "                    name='ASX 200',visible=\"legendonly\"),secondary_y=True)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=self.SP500.Date, y=self.SP500['Price'],\n",
    "            mode='lines',\n",
    "            name='SP500',visible=\"legendonly\"),secondary_y=True)\n",
    "        \n",
    "        fig.add_trace(go.Scatter(x=self.AUD_index.Date, y=self.AUD_index['Price'],\n",
    "            mode='lines',\n",
    "            name='Scaled AUD Index',visible=\"legendonly\"),secondary_y=True)\n",
    "        \n",
    "        fig.data[1].update(xaxis='x2')\n",
    "        fig.data[2].update(xaxis='x2')\n",
    "        fig.data[3].update(xaxis='x2')\n",
    "        fig.data[4].update(xaxis='x2')\n",
    "        \n",
    "        fig.update_layout(yaxis_title =\"Standardised sentiment score\",width=1000, height=475,template=\"plotly_white\",)\n",
    "        fig.update_xaxes(dict(\n",
    "                    tickmode = 'array',\n",
    "                    tickvals = self.by_week.week_of_year,\n",
    "                    ticktext = self.by_week.week_strt_day\n",
    "                ), row=1, col=1)\n",
    "        fig.update_yaxes(title_text =\"Stock market index\",\n",
    "                       secondary_y = True)\n",
    "        fig.write_json(\"graph_json/stockMarket_line.json\")\n",
    "#         plt_div = plotly.offline.plot(fig,output_type='div')\n",
    "#         return plt_div \n",
    "#         fig.write_html(\"Chart_HTML/Covid-related_glance/stockMarket_line.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class map:\n",
    "    def __init__(self,city_mean_score):\n",
    "        with open('city_mean.geojson') as response:\n",
    "            self.cities_id = json.load(response)\n",
    "            \n",
    "        with open('regional_geo.geojson') as response:\n",
    "            self.region_id = json.load(response)    \n",
    "            \n",
    "        self.city_mean_score = city_mean_score.toPandas()\n",
    "        \n",
    "        def to_geopoint(coordinates):\n",
    "            return geometry.Point(coordinates)\n",
    "        \n",
    "        townGeo = pd.read_json(\"2020Mean_cityGeo.json\")\n",
    "        townGeo['geometry'] = townGeo['geometry'].apply(to_geopoint)\n",
    "        self.townGeo = gpd.GeoDataFrame(townGeo,geometry=townGeo.geometry)\n",
    "        self.townGeo.crs={'init': 'epsg:4326'}\n",
    "        \n",
    "        self.age_regional = gpd.read_file(\"age_regional.geojson\")\n",
    "        \n",
    "        VIC = gpd.read_file('australia_administrative_victoria_boundary/australia_administrative_victoria_boundary.shp')[['name','geometry']]\n",
    "        VIC.crs={'init': 'epsg:4326'}\n",
    "        self.VIC = VIC.drop(0)\n",
    "        \n",
    "        self.VIC_regional = gpd.read_file('australia_administrative_boundaries_level6_counties_polygon/australia_administrative_boundaries_level6_counties_polygon.shp')[['name','geometry']]\n",
    "        self.VIC_regional.crs={'init': 'epsg:4326'}\n",
    "        region_join = sjoin(self.VIC_regional,self.VIC,\"inner\")\n",
    "        self.region_join = region_join.rename(columns={'name_left':'city','name_right':'suburb'})\n",
    "        \n",
    "        age_dis = pd.read_json(\"peopleAge_distribution2020.json\")\n",
    "        def clean_name(name):\n",
    "            l = name.split('(')[0].strip(\" \")\n",
    "            return l\n",
    "        age_dis['lga_name'] = age_dis['lga_name'].apply(clean_name)\n",
    "        names = age_dis.columns\n",
    "        new = []\n",
    "        for i in range(len(names)):\n",
    "            if names[i].startswith(\"_\"):\n",
    "                name = names[i][1:]\n",
    "                name = name.replace(\"_yrs_proj\",\"\")\n",
    "                new.append(name)\n",
    "            else:\n",
    "                new.append(names[i])\n",
    "        age_dis.columns = new\n",
    "\n",
    "        age_dis = age_dis.melt(id_vars=[\"lga_name\",'lga_code','tot_proj_pop_denom','ste_name'], \n",
    "                var_name=\"age_group\", \n",
    "                value_name=\"population\")\n",
    "\n",
    "        age_dis['contain'] = age_dis['age_group'].str.contains('count')\n",
    "        age_dis = age_dis[age_dis['contain']==True]\n",
    "        age_dis['age_group'] = age_dis['age_group'].str.replace(\"_count\",\"\")\n",
    "        age_dis['age_group'] = age_dis['age_group'].str.replace(\"_\",\"-\")\n",
    "        age_dis['age_group'] = age_dis['age_group'].str.replace(\"85-yrs-over-proj\",\"over 85\")\n",
    "        self.age_dis = age_dis.drop(['ste_name','contain'],axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def payroll(self,city):\n",
    "        payroll = pd.read_json(\"weekly_payroll2020.json\")\n",
    "        payroll = payroll.drop(['sa3_code16','sa4_code16','ogc_fid','ste','sa3_name16'],axis=1)\n",
    "        \n",
    "        new_names = []\n",
    "        for column in payroll.columns:\n",
    "            if \"wk_end_\" in column:\n",
    "                name = column\n",
    "                name = name.replace(\"wk_end_\",\"\")\n",
    "                name = name.replace(\"_\",\"-\")\n",
    "                new_names.append(name)\n",
    "\n",
    "            else:\n",
    "                new_names.append(column)\n",
    "\n",
    "        payroll.columns = new_names\n",
    "\n",
    "        payroll = payroll.melt(id_vars=[\"sa4_name16\"], \n",
    "                var_name=\"week\", \n",
    "                value_name=\"payroll\")\n",
    "        payroll['week'] = pd.to_datetime(payroll['week']).dt.date\n",
    "        \n",
    "        payroll_gpd = gpd.read_file('payroll_geo.geojson',geometry=geometry)\n",
    "\n",
    "        payroll_VIC = sjoin(self.VIC,payroll_gpd,how='inner')[['name','sa4_name16']]\n",
    "        payroll_by_city = payroll.merge(payroll_VIC,left_on=\"sa4_name16\",right_on=\"sa4_name16\",how=\"inner\")\n",
    "        payroll_by_city = payroll_by_city.groupby(by=['name','week']).mean().reset_index()\n",
    "        payroll_mean = payroll_by_city.groupby(by=['week']).mean().reset_index()\n",
    "        payroll_mean = payroll_mean.assign(name='Average in VIC')\n",
    "        payroll_city = payroll_by_city[payroll_by_city['name']==city]\n",
    "        payroll_city = pd.concat([payroll_city,payroll_mean])\n",
    "        payroll_city.columns = ['Region','Date','Average payroll']\n",
    "        fig = px.line( payroll_city, x=\"Date\", y=\"Average payroll\", color=\"Region\",\n",
    "              line_group=\"Region\", hover_name=\"Region\")\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                    'text':\"Average payroll in {} during Covid-19 pandemic\".format(city),\n",
    "                    'y':0.98,\n",
    "                    'x':0.5,\n",
    "                    'xanchor': 'center',\n",
    "                    'yanchor': 'top'},\n",
    "             autosize=False,\n",
    "                width=650,\n",
    "            template=\"plotly_white\",\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            )\n",
    "            )\n",
    "\n",
    "#         fig.write_html(\"Chart_HTML/RegionalSentiment/Payroll_LineGraph/{}.html\".format(city))\n",
    "        fig.write_json(\"graph_json/payroll_line_{}.json\".format(city))\n",
    "\n",
    "    def age_distrubion(self, city,color):\n",
    "#         cities_id = gpd.GeoDataFrame(self.cities_id,geometry=self.cities_id['geometry'])\n",
    "        age_VIC = sjoin(self.VIC,self.age_regional)\n",
    "        age_geo = self.age_dis.merge(age_VIC,left_on=\"lga_name\",right_on=\"lga_name\",how=\"inner\")\n",
    "        \n",
    "        age_by_city = age_geo[['name','age_group','population']].groupby(by=['name','age_group']).mean().reset_index().sort_values(by=['name','age_group'])\n",
    "        age_by_city['population'] = age_by_city['population'].astype(int)\n",
    "        age_city = age_by_city[age_by_city['name']==city]\n",
    "\n",
    "        def group_age(group):\n",
    "            if group in ['0-4','5-9','10-14']:\n",
    "                return 'Children: 0-14'\n",
    "            elif group == '15-19':\n",
    "                return 'Teenager: 15-19'\n",
    "            elif group in ['20-24','25-29','30-34', '35-39']:\n",
    "                return 'Adult: 20-39'\n",
    "            elif group in ['40-44','45-49','50-54','55-59']:\n",
    "                return 'Middle-aged: 40-59'\n",
    "            else:\n",
    "                return 'Elderly: above 60'\n",
    "        age_city['age_class'] = age_city['age_group'].apply(group_age)\n",
    "        \n",
    "        labels = ['Children: 0-14','Teenager: 15-19','Adult: 20-39','Middle-aged: 40-59','Elderly: above 60']\n",
    "        title = \"<b>Distribution of age groups<b> \\n in {}<b>\".format(city)\n",
    "        age_city = age_city[['age_class','population']].groupby(by='age_class').sum()\n",
    "        age_city = age_city.reindex(labels).reset_index()\n",
    "\n",
    "        annotations=[]\n",
    "        annotations.append(dict(xref='paper', yref='paper',\n",
    "          x=0.5, y=0.5,\n",
    "          xanchor= 'center',\n",
    "          yanchor='middle',\n",
    "          text= '<b>{}<b>'.format(city),\n",
    "          font=dict(family=\"Arial\", size=12),\n",
    "          showarrow=False,\n",
    "          ))\n",
    "\n",
    "        fig = go.Figure(data=[go.Pie(labels=age_city['age_class'], values=age_city['population'], hole=.4,\n",
    "                                     direction ='clockwise', sort=False,\n",
    "                                     marker=dict(colors=color,\n",
    "                                                line=dict(color='white',width=2)),\n",
    "                                    )])\n",
    "\n",
    "        fig.update_traces(\n",
    "          textposition='inside',\n",
    "          textinfo='value+percent',\n",
    "          showlegend=True,\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "        title={\n",
    "                'text':\"Distribution of population age groups in 2020\",\n",
    "                'y':0.95,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'},\n",
    "         annotations=annotations,\n",
    "         autosize=False,\n",
    "            width=650,\n",
    "            height=650,\n",
    "        )\n",
    "\n",
    "#         fig.write_html(\"Chart_HTML/RegionalSentiment/AgeDistribution_PieChart/{}.html\".format(city))\n",
    "        fig.write_json(\"graph_json/age_pie_{}.json\".format(city))\n",
    "    def map_with_suburb(self):        \n",
    "        city_join = sjoin(self.townGeo,self.VIC_regional,how=\"left\")[['city','name']].sort_values(by=['name'])\n",
    "        city_mean = self.city_mean_score.merge(city_join,left_on='city',right_on='city')\n",
    "        city_mean = city_mean.groupby(['name','month']).mean().sort_values(by=['name','month']).reset_index()\n",
    "        overall_mean = city_mean.rename(columns={\"avg(sentiment_score)\":\"score\"})\n",
    "#         overall_mean['score'] = preprocessing.scale(overall_mean['score'])\n",
    "        overall_mean  = overall_mean.merge(self.region_join,left_on=\"name\",right_on=\"city\",how=\"inner\")\n",
    "#         overall_mean['month'] = overall_mean['month'].replace({3:\"March\",4:\"April\",5:\"May\",6:\"June\",7:\"July\",8:\"August\",9:\"September\",10:\"October\",11:\"November\",12:\"December\"})\n",
    "#         overall_mean = overall_mean[overall_mean['month']==mon]\n",
    "#         overall_mean_geo = gpd.GeoDataFrame(overall_mean,geometry=overall_mean.geometry)\n",
    "#         overall_mean_geo.to_file('regional_geo.geojson', driver=\"GeoJSON\")\n",
    "\n",
    "        fig = px.choropleth_mapbox(overall_mean, geojson=self.region_id, locations='name', color='score',\n",
    "                                   color_continuous_scale=\"balance\",\n",
    "                                   range_color=(-0.5,0.5),\n",
    "                                   mapbox_style=\"carto-positron\",\n",
    "                                   zoom=5.5, center = {\"lat\": -36, \"lon\": 144.9631},\n",
    "                                   opacity=0.5,\n",
    "                                   labels={'score':'Scaled sentiment score'})\n",
    "\n",
    "        fig.show()\n",
    "#         fig.write_html(\"Chart_HTML/RegionalSentiment/sentimentMap_bySuburb/sentimentMap_bySuburb_{}.html\".format(mon))\n",
    "#         return overall_mean\n",
    "#         fig.write_json(\"graph_json/Map_bySuburb_{}.json\".format(mon))\n",
    "    def map_with_city(self):\n",
    "        city_join = sjoin(self.townGeo,self.VIC,how=\"right\")[['city','name']]\n",
    "        city_mean = self.city_mean_score.merge(city_join,left_on='city',right_on='city')\n",
    "        city_mean = city_mean.groupby(['name','month']).mean().sort_values(by=['name','month']).reset_index()\n",
    "        overall_mean = city_mean.rename(columns={\"avg(sentiment_score)\":\"score\"})\n",
    "#         overall_mean['score'] = preprocessing.scale(overall_mean['score'])\n",
    "        overall_mean  = overall_mean.merge(self.VIC,left_on=\"name\",right_on=\"name\",how=\"left\")\n",
    "        overall_mean_geo = gpd.GeoDataFrame(overall_mean,geometry=overall_mean.geometry)\n",
    "        fig = px.choropleth_mapbox(overall_mean_geo, geojson=self.cities_id, locations='name', color='score',\n",
    "                                   color_continuous_scale=\"balance\",\n",
    "                                   range_color=(-0.2,0.2),\n",
    "                                   mapbox_style=\"carto-positron\",\n",
    "                                   zoom=5.5, center = {\"lat\": -36, \"lon\": 144.9631},\n",
    "                                   opacity=0.5,\n",
    "                                   labels={'score':'Scaled sentiment score'}\n",
    "                                  )\n",
    "        fig.write_json(\"graph_json/Map_byCity.json\")\n",
    "#         fig.write_html(\"Chart_HTML/RegionalSentiment/sentimentMap_byCity.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e7491",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_json(\"2020historical/*.json\")\n",
    "df = spark_df(data)\n",
    "SA = Sentiment(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8de32",
   "metadata": {},
   "source": [
    "# Covid-19 in Australia at a glance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d928afc",
   "metadata": {},
   "source": [
    "## Line graph of Covid cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238edff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b812e623",
   "metadata": {},
   "source": [
    "### Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e25b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_analysis = Topic_analysis(df)\n",
    "topic_analysis.word_cloud_fall(\"March\")\n",
    "# topic_analysis.word_cloud_fall(\"April\")\n",
    "# topic_analysis.word_cloud_fall(\"May\")\n",
    "# topic_analysis.word_cloud_winter(\"June\")\n",
    "# topic_analysis.word_cloud_winter(\"July\")\n",
    "# topic_analysis.word_cloud_winter(\"August\")\n",
    "# topic_analysis.word_cloud_spring(\"September\")\n",
    "# topic_analysis.word_cloud_spring(\"October\")\n",
    "# topic_analysis.word_cloud_summer(\"November\")\n",
    "# topic_analysis.word_cloud_summer(\"December\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c001f",
   "metadata": {},
   "source": [
    "### Barchart of covid-related tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SA = Sentiment(df)\n",
    "SA.covid_overall_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3730ad",
   "metadata": {},
   "source": [
    "## Topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_analysis = Topic_analysis(df)\n",
    "formatted=topic_analysis.lda()\n",
    "py_lda_prepared_data = pyLDAvis.prepare(formatted['topic_term_dists'],formatted['doc_topic_dists'],formatted['doc_lengths'],formatted['vocab'],formatted['term_frequency'])\n",
    "# pyLDAvis.display(py_lda_prepared_data)\n",
    "pyLDAvis.save_html(py_lda_prepared_data , 'Chart_HTML/Covid-related_glance/lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39feaa6a",
   "metadata": {},
   "source": [
    "### Top reactive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_analysis.top_react_tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c33b1",
   "metadata": {},
   "source": [
    "# Regional sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mean_score = SA.by_city()\n",
    "sentiment_map = map(city_mean_score)\n",
    "sentiment_map.map_with_suburb()\n",
    "# sentiment_map.map_with_suburb('April')\n",
    "# sentiment_map.map_with_suburb('May')\n",
    "# sentiment_map.map_with_suburb('June')\n",
    "# sentiment_map.map_with_suburb('July')\n",
    "# sentiment_map.map_with_suburb('August')\n",
    "# sentiment_map.map_with_suburb('September')\n",
    "# sentiment_map.map_with_suburb('October')\n",
    "# sentiment_map.map_with_suburb('November')\n",
    "# sentiment_map.map_with_suburb('December')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b8ee6",
   "metadata": {},
   "source": [
    "### Line graph of payroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_mean_score = SA.by_city()\n",
    "sentiment_map = map(city_mean_score)\n",
    "sentiment_map.payroll(\"Hume\")\n",
    "sentiment_map.payroll(\"Loddon Mallee\")\n",
    "sentiment_map.payroll(\"Grampians\")\n",
    "sentiment_map.payroll(\"Barwon South West\")\n",
    "sentiment_map.payroll(\"Gippsland\")\n",
    "sentiment_map.payroll(\"Greater Melbourne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payroll = pd.read_json(\"weekly_payroll2020.json\")\n",
    "# payroll = payroll.drop(['sa3_code16','sa4_code16','ogc_fid','sa3_name16'],axis=1)\n",
    "# new_names = []\n",
    "# for column in payroll.columns:\n",
    "#     if \"wk_end_\" in column:\n",
    "#         name = column\n",
    "#         name = name.replace(\"wk_end_\",\"\")\n",
    "#         name = name.replace(\"_\",\"-\")\n",
    "#         new_names.append(name)\n",
    "\n",
    "#     else:\n",
    "#         new_names.append(column)\n",
    "\n",
    "# payroll.columns = new_names\n",
    "\n",
    "# payroll = payroll.melt(id_vars=[\"sa4_name16\"], \n",
    "#         var_name=\"week\", \n",
    "#         value_name=\"payroll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759ff9e",
   "metadata": {},
   "source": [
    "### Pie chart of age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_dis = pd.read_json(\"peopleAge_distribution2020.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = map(city_mean_score)\n",
    "sentiment_map.age_distrubion(\"Hume\",px.colors.sequential.Burgyl)\n",
    "sentiment_map.age_distrubion(\"Loddon Mallee\",px.colors.sequential.Burgyl)\n",
    "sentiment_map.age_distrubion(\"Grampians\",px.colors.sequential.Burgyl)\n",
    "sentiment_map.age_distrubion(\"Barwon South West\",px.colors.sequential.Burgyl)\n",
    "sentiment_map.age_distrubion(\"Gippsland\",px.colors.sequential.Burgyl)\n",
    "sentiment_map.age_distrubion(\"Greater Melbourne\",px.colors.sequential.Burgyl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09898ad6",
   "metadata": {},
   "source": [
    "### GoogleMap API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54d56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from datetime import datetime\n",
    "\n",
    "gmaps = googlemaps.Client(key='AIzaSyB47aIQkask-naqcK7kCGHZjxtSJsZe-3k')\n",
    "def age_geo(city):\n",
    "    try:\n",
    "        name = city+\", AU\"\n",
    "        geocode_result = gmaps.geocode(name)\n",
    "    #     lng = geocode_result[0]['geometry']['location']['lng']\n",
    "    #     lat = geocode_result[0]['geometry']['location']['lat']\n",
    "        return geocode_result\n",
    "    except:\n",
    "        return\n",
    "\n",
    "results['geo'] = results['sa4_name16'].apply(age_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db615163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = age_region.iloc[0]['geo']\n",
    "def get_Point(row):\n",
    "    lat = 0\n",
    "    lng = 0\n",
    "    for i in row:\n",
    "        lat = i['geometry']['location']['lat']\n",
    "        lng = i['geometry']['location']['lng']\n",
    "    return geometry.Point(lng,lat)\n",
    "results['geoPoint'] = results['geo'].apply(get_Point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59914be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = payroll[['sa4_name16','week','payroll','geoPoint']]\n",
    "results = results[['sa4_name16','geometry']]\n",
    "results_gpd = gpd.GeoDataFrame(results,geometry=results.geometry)\n",
    "results_gpd.to_file('payroll_geo.geojson', driver=\"GeoJSON\") \n",
    "# results.to_file('regional_geo.geojson', driver=\"GeoJSON\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "payroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_regional_gpd = gpd.GeoDataFrame(age_region[['lga_name','geometry']],geometry=age_region.geometry)\n",
    "# age_regional_gpd.to_file('age_regional.geojson', driver=\"GeoJSON\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931988f",
   "metadata": {},
   "source": [
    "# Relationships between public sentiment & stock market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_results = SA.sentiment_results()\n",
    "sentiment_impact(SA_results).overall()\n",
    "# chart_dic = {'stock_line':sentiment_impact(SA_results).overall()}\n",
    "# with open('person.json', 'w') as json_file:\n",
    "#     json.dump(chart_dic, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72e328",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fafc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('regional_geo.geojson') as response:\n",
    "    cities_id = json.load(response)\n",
    "results = region[['name','geometry']]\n",
    "results = gpd.GeoDataFrame(results ,geometry=results.geometry)\n",
    "results=results.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_file('regional_geo.geojson', driver=\"GeoJSON\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('regional_geo.geojson') as file:\n",
    "    geo = json.load(file)\n",
    "for i in geo['features']:\n",
    "    i['id'] = i['properties']['name']\n",
    "# geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4193ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('regional_geo.geojson','w') as file:\n",
    "    json.dump(geo, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d25bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efbe022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cur_path = os.path.dirname(__file__)\n",
    "new_path = os.path.relpath('..\\\\stock_line.json', cur_path)\n",
    "with open(new_path, 'r') as json_file:\n",
    "    plots = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import json\n",
    "import plotly\n",
    "with open('payroll_line_Barwon South West.json', 'r') as json_file:\n",
    "#     plots = json.load(json_file)\n",
    "    print(json_file)\n",
    "# pio.show(plots)\n",
    "# plt_div = plotly.offline.plot(plots,output_type='div')\n",
    "# pio.show(plt_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf303d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
